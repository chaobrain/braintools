# Copyright 2024 BDP Ecosystem Limited. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

"""Checkpointing helper functions.

This module is rewritten from the Flax APIs (https://github.com/google/flax).
"""

import enum
import os
import sys
import threading
import warnings
from concurrent.futures import thread
from contextlib import contextmanager
from typing import Any, Callable, Dict, List, Optional

import brainstate
import brainunit as u
import jax
import numpy as np

from .._misc import set_module_as

try:
    import msgpack
except (ModuleNotFoundError, ImportError):
    msgpack = None

__all__ = [
    'msgpack_from_state_dict',
    'msgpack_to_state_dict',
    'msgpack_register_serialization',
    'msgpack_save',
    'msgpack_load',
    'AsyncManager',
]


class AlreadyExistsError(Exception):
    __module__ = 'braintools.file'
    """Attempting to overwrite a file via copy.

    You can pass ``overwrite=True`` to disable this behavior and overwrite
    existing files in.
    """

    def __init__(self, path):
        super().__init__(f'Trying overwrite an existing file: "{path}".')


class InvalidCheckpointPath(Exception):
    __module__ = 'braintools.file'
    """A checkpoint cannot be stored in a directory that already has

    a checkpoint at the current or a later step.

    You can pass ``overwrite=True`` to disable this behavior and
    overwrite existing checkpoints in the target directory.
    """

    def __init__(self, path):
        super().__init__(f'Invalid checkpoint at "{path}".')


# msgpack has a hard limit of 2**31 - 1 bytes per object leaf.  To circumvent
# this limit for giant arrays (e.g. embedding tables), we traverse the tree
# and break up arrays near the limit into flattened array chunks.
# True limit is 2**31 - 1, but leave a margin for encoding padding.
MAX_CHUNK_SIZE = 2 ** 30

_STATE_DICT_REGISTRY: Dict[Any, Any] = {}


class _ErrorContext(threading.local):
    __module__ = 'braintools.file'
    """Context for deserialization error messages."""

    def __init__(self):
        self.path = []


_error_context = _ErrorContext()


@contextmanager
def _record_path(name):
    try:
        _error_context.path.append(name)
        yield
    finally:
        _error_context.path.pop()


@set_module_as('braintools.file')
def check_msgpack():
    if msgpack is None:
        raise ModuleNotFoundError('\nPlease install msgpack via:\n'
                                  '> pip install msgpack')


@set_module_as('braintools.file')
def current_path():
    """Current state_dict path during deserialization for error messages."""
    return '/'.join(_error_context.path)


class _NamedTuple:
    __module__ = 'braintools.file'
    """Fake type marker for namedtuple for registry."""
    pass


@set_module_as('braintools.file')
def _is_namedtuple(x):
    """Duck typing test for namedtuple factory-generated objects."""
    return isinstance(x, tuple) and hasattr(x, '_fields')


@set_module_as('braintools.file')
def msgpack_from_state_dict(
    target,
    state: Dict[str, Any],
    name: str = '.',
    mismatch: str = 'error'
):
    r"""Restore the state of the given target using a state dictionary.

    This function reconstructs an object's state from a serialized state dictionary,
    preserving the exact structure, shapes, and dtypes of the original target.
    It serves as the counterpart to `msgpack_to_state_dict` for deserialization.

    The function performs type-aware reconstruction by matching the target's type
    with registered serialization handlers in the `_STATE_DICT_REGISTRY`.

    Parameters
    ----------
    target : Any
        The template object whose state should be restored. This provides the
        structural blueprint and type information for reconstruction.
    state : Dict[str, Any]
        State dictionary generated by `msgpack_to_state_dict` containing the
        serialized state data to restore.
    name : str, default='.'
        Name of the current branch being processed, used for detailed error
        messages during deserialization.
    mismatch : {'error', 'warn', 'ignore'}, default='error'
        How to handle mismatches between target structure and state dict:

        - ``'error'``: Raise ValueError on any mismatch
        - ``'warn'``: Issue warning and skip mismatched keys
        - ``'ignore'``: Silently skip mismatched keys

    Returns
    -------
    Any
        A copy of the object with state restored from the state dictionary.
        The returned object has the same type and structure as the target
        but with updated state values.

    Examples
    --------
    Basic state restoration:

    .. code-block:: python

        >>> import braintools
        >>> import jax.numpy as jnp
        >>> # Original data structure
        >>> original = {'weights': jnp.array([1.0, 2.0]), 'bias': 0.5}
        >>> # Serialize to state dict
        >>> state_dict = braintools.file.msgpack_to_state_dict(original)
        >>> # Create target template
        >>> target = {'weights': jnp.zeros(2), 'bias': 0.0}
        >>> # Restore state
        >>> restored = braintools.file.msgpack_from_state_dict(target, state_dict)
        >>> print(restored['weights'])
        [1. 2.]

    Handling mismatches:

    .. code-block:: python

        >>> # Target with different structure
        >>> target_mismatch = {'weights': jnp.zeros(2)}  # Missing 'bias'
        >>> # Restore with warning on mismatch
        >>> restored = braintools.file.msgpack_from_state_dict(
        ...     target_mismatch, state_dict, mismatch='warn'
        ... )

    See Also
    --------
    msgpack_to_state_dict : Convert object to state dictionary
    msgpack_register_serialization : Register custom type serialization

    Notes
    -----
    The function uses a registry-based approach for type-specific deserialization.
    Custom types must be registered using `msgpack_register_serialization`
    before they can be properly restored.
    """
    ty = _NamedTuple if _is_namedtuple(target) else type(target)
    for t in _STATE_DICT_REGISTRY.keys():
        if issubclass(ty, t):
            ty = t
            break
    else:
        return state
    ty_from_state_dict = _STATE_DICT_REGISTRY[ty][1]
    with _record_path(name):
        return ty_from_state_dict(target, state, mismatch)


@set_module_as('braintools.file')
def msgpack_to_state_dict(target) -> Dict[str, Any]:
    r"""Convert an object to a serializable state dictionary.

    This function creates a serializable representation of the given target object
    by extracting its state into a dictionary format. It serves as the primary
    serialization entry point for the msgpack-based checkpointing system.

    The function uses a registry-based approach to handle different types,
    delegating to type-specific serialization handlers registered in
    `_STATE_DICT_REGISTRY`.

    Parameters
    ----------
    target : Any
        The object to be serialized. Can be any Python object with a registered
        serialization handler, including nested structures like lists, dicts,
        namedtuples, JAX arrays, and custom objects.

    Returns
    -------
    Dict[str, Any]
        A dictionary representation of the target's state that can be serialized
        to msgpack format. For unregistered types, returns the object unchanged.

    Examples
    --------
    Serialize basic data structures:

    .. code-block:: python

        >>> import braintools
        >>> import jax.numpy as jnp
        >>> # Serialize a dictionary with arrays
        >>> data = {'weights': jnp.array([1.0, 2.0]), 'bias': 0.5}
        >>> state_dict = braintools.file.msgpack_to_state_dict(data)
        >>> print(type(state_dict))
        <class 'dict'>

    Serialize complex nested structures:

    .. code-block:: python

        >>> # Nested structure with lists and arrays
        >>> nested = {
        ...     'layer1': {'weights': jnp.ones((2, 3)), 'bias': jnp.zeros(3)},
        ...     'layer2': {'weights': jnp.ones((3, 1)), 'bias': jnp.zeros(1)}
        ... }
        >>> state_dict = braintools.file.msgpack_to_state_dict(nested)
        >>> print('layer1' in state_dict)
        True

    See Also
    --------
    msgpack_from_state_dict : Restore object from state dictionary
    msgpack_register_serialization : Register custom type serialization
    msgpack_save : Save state dictionary to file

    Notes
    -----
    For dictionary state dicts, all keys must be strings. The function validates
    this requirement and raises an assertion error if non-string keys are found.

    Custom types must be registered using `msgpack_register_serialization`
    before they can be properly serialized.
    """
    ty = _NamedTuple if _is_namedtuple(target) else type(target)

    for t in _STATE_DICT_REGISTRY.keys():
        if issubclass(ty, t):
            ty = t
            break
    else:
        return target

    ty_to_state_dict = _STATE_DICT_REGISTRY[ty][0]
    state_dict = ty_to_state_dict(target)
    if isinstance(state_dict, dict):
        for key in state_dict.keys():
            assert isinstance(key, str), 'A state dict must only have string keys.'
        return state_dict
    else:
        return state_dict


@set_module_as('braintools.file')
def msgpack_register_serialization(
    ty,
    ty_to_state_dict,
    ty_from_state_dict,
    override: bool = False
):
    r"""Register a custom type for msgpack serialization and deserialization.

    This function allows users to register custom serialization handlers for
    specific types, enabling them to be properly saved and restored using
    the msgpack checkpointing system.

    Parameters
    ----------
    ty : type
        The type class to be registered for serialization.
    ty_to_state_dict : Callable[[ty], Dict[str, Any]]
        Function that takes an instance of `ty` and returns its state as a
        dictionary. This function should extract all necessary information
        to reconstruct the object.
    ty_from_state_dict : Callable[[ty, Dict[str, Any]], ty]
        Function that takes an instance of `ty` and a state dictionary,
        and returns a copy of the instance with the restored state.
    override : bool, default=False
        Whether to override a previously registered serialization handler
        for the same type.

    Raises
    ------
    ValueError
        If a serialization handler for the type is already registered
        and `override` is False.

    Examples
    --------
    Register a custom class for serialization:

    .. code-block:: python

        >>> import braintools
        >>> class MyModel:
        ...     def __init__(self, weights, bias):
        ...         self.weights = weights
        ...         self.bias = bias
        ...
        >>> def model_to_dict(model):
        ...     return {'weights': model.weights, 'bias': model.bias}
        ...
        >>> def model_from_dict(model, state_dict):
        ...     return MyModel(state_dict['weights'], state_dict['bias'])
        ...
        >>> braintools.file.msgpack_register_serialization(
        ...     MyModel, model_to_dict, model_from_dict
        ... )

    Use the registered type in checkpointing:

    .. code-block:: python

        >>> import jax.numpy as jnp
        >>> model = MyModel(jnp.array([1.0, 2.0]), 0.5)
        >>> # Now MyModel can be saved and loaded
        >>> braintools.file.msgpack_save('model.msgpack', model)
        >>> loaded_model = braintools.file.msgpack_load('model.msgpack', model)

    See Also
    --------
    msgpack_to_state_dict : Convert object to state dictionary
    msgpack_from_state_dict : Restore object from state dictionary
    msgpack_save : Save objects to file
    msgpack_load : Load objects from file

    Notes
    -----
    The registration is global and persists for the duration of the Python session.
    State dictionaries should only contain string keys for compatibility with
    the msgpack format.

    For complex objects with nested structures, ensure that all nested types
    are also properly registered or are built-in supported types.
    """
    if ty in _STATE_DICT_REGISTRY and not override:
        raise ValueError(f'a serialization handler for "{ty.__name__}"'
                         ' is already registered')
    _STATE_DICT_REGISTRY[ty] = (ty_to_state_dict, ty_from_state_dict)


@set_module_as('braintools.file')
def _list_state_dict(xs: List[Any]) -> Dict[str, Any]:
    return {
        str(i): msgpack_to_state_dict(x)
        for i, x in enumerate(xs)
    }


@set_module_as('braintools.file')
def _restore_list(xs, state_dict: Dict[str, Any], mismatch: str = 'error') -> List[Any]:
    if len(state_dict) != len(xs):
        msg = ('The size of the list and the state dict do not match,'
               f' got {len(xs)} and {len(state_dict)} '
               f'at path {current_path()}')
        if mismatch == 'error':
            raise ValueError(msg)
        elif mismatch == 'warn':
            warnings.warn(msg, UserWarning)
        # For 'ignore', continue with available data
        elif mismatch == 'ignore':
            pass
        else:
            raise ValueError(msg)

    ys = []
    max_len = min(len(xs), len(state_dict))
    for i in range(max_len):
        y = msgpack_from_state_dict(xs[i], state_dict[str(i)], name=str(i), mismatch=mismatch)
        ys.append(y)

    # If target is longer, pad with original values
    for i in range(max_len, len(xs)):
        ys.append(xs[i])

    return ys


msgpack_register_serialization(
    list,
    _list_state_dict,
    _restore_list,
)
msgpack_register_serialization(
    tuple,
    _list_state_dict,
    lambda xs, state_dict, mismatch='error': tuple(_restore_list(list(xs), state_dict, mismatch))
)


@set_module_as('braintools.file')
def _dict_state_dict(xs: Dict[str, Any]) -> Dict[str, Any]:
    if isinstance(xs, brainstate.util.FlattedDict):
        xs = xs.to_nest()
    str_keys = set(str(k) for k in xs.keys())
    if len(str_keys) != len(xs):
        raise ValueError('Dict keys do not have a unique string representation: '
                         f'{str_keys} vs given: {xs}')
    return {
        str(key): msgpack_to_state_dict(value)
        for key, value in xs.items()
    }


@set_module_as('braintools.file')
def _restore_dict(xs, states: Dict[str, Any], mismatch: str = 'error') -> Dict[str, Any]:
    if isinstance(xs, brainstate.util.FlattedDict):
        xs = xs.to_nest()
    if isinstance(states, brainstate.util.FlattedDict):
        states = states.to_nest()
    diff = set(map(str, xs.keys())).difference(states.keys())
    if diff:
        msg = ('The target dict keys and state dict keys do not match,'
               f' target dict contains keys {diff} which are not present in state dict '
               f'at path {current_path()}')
        if mismatch == 'error':
            raise ValueError(msg)
        elif mismatch == 'warn':
            warnings.warn(msg, UserWarning)
        # For 'ignore', continue with available keys
        elif mismatch == 'ignore':
            pass
        else:
            raise ValueError(msg)

    result = {}
    for key, value in xs.items():
        str_key = str(key)
        if str_key in states:
            result[key] = msgpack_from_state_dict(value, states[str_key], name=str_key, mismatch=mismatch)
        else:
            # Keep original value if key is missing from state_dict
            result[key] = value

    return result


msgpack_register_serialization(dict, _dict_state_dict, _restore_dict)


@set_module_as('braintools.file')
def _namedtuple_state_dict(nt) -> Dict[str, Any]:
    return {key: msgpack_to_state_dict(getattr(nt, key)) for key in nt._fields}


@set_module_as('braintools.file')
def _restore_namedtuple(xs, state_dict: Dict[str, Any], mismatch: str = 'error'):
    """Rebuild namedtuple from serialized dict."""
    if set(state_dict.keys()) == {'name', 'fields', 'values'}:
        state_dict = {state_dict['fields'][str(i)]: state_dict['values'][str(i)]
                      for i in range(len(state_dict['fields']))}

    sd_keys = set(state_dict.keys())
    nt_keys = set(xs._fields)

    if sd_keys != nt_keys:
        msg = ('The field names of the state dict and the named tuple do not match,'
               f' got {sd_keys} and {nt_keys} at path {current_path()}')
        if mismatch == 'error':
            raise ValueError(msg)
        elif mismatch == 'warn':
            warnings.warn(msg, UserWarning)
        # For 'ignore', continue with available fields
        elif mismatch == 'ignore':
            pass
        else:
            raise ValueError(msg)

    fields = {}
    for field in xs._fields:
        if field in state_dict:
            fields[field] = msgpack_from_state_dict(getattr(xs, field), state_dict[field], name=field,
                                                    mismatch=mismatch)
        else:
            # Keep original value if field is missing from state_dict
            fields[field] = getattr(xs, field)

    return type(xs)(**fields)


msgpack_register_serialization(
    _NamedTuple,
    _namedtuple_state_dict,
    _restore_namedtuple
)


@set_module_as('braintools.file')
def _quantity_dict_state(x: u.Quantity) -> Dict[str, jax.Array]:
    return {
        'mantissa': x.mantissa,
        'scale': x.unit.scale,
        'base': x.unit.base,
        'dim': x.unit.dim._dims,
        'factor': x.unit.factor,
    }


@set_module_as('braintools.file')
def _restore_quantity(x: u.Quantity, state_dict: Dict, mismatch: str = 'error') -> u.Quantity:
    unit = u.Unit(
        dim=u.Dimension(state_dict['dim']),
        scale=state_dict['scale'],
        base=state_dict['base'],
        factor=state_dict['factor']
    )
    if x.unit != unit:
        msg = f'Unit mismatch: expected {x.unit}, got {unit} at path {current_path()}'
        if mismatch == 'error':
            raise ValueError(msg)
        elif mismatch == 'warn':
            warnings.warn(msg, UserWarning)
        elif mismatch == 'ignore':
            pass
        else:
            raise ValueError(msg)
        # For 'ignore', use the loaded unit
    return u.Quantity(state_dict['mantissa'], unit=unit)


msgpack_register_serialization(u.Quantity, _quantity_dict_state, _restore_quantity)


@set_module_as('braintools.file')
def _brainstate_dict_state(x: brainstate.State) -> Dict[str, Any]:
    return msgpack_to_state_dict(x.value)


@set_module_as('braintools.file')
def _restore_brainstate(x: brainstate.State, state_dict: Dict, mismatch: str = 'error') -> brainstate.State:
    x.value = msgpack_from_state_dict(x.value, state_dict, mismatch=mismatch)
    return x


msgpack_register_serialization(brainstate.State, _brainstate_dict_state, _restore_brainstate)

msgpack_register_serialization(
    jax.tree_util.Partial,
    lambda x: {
        "args": msgpack_to_state_dict(x.args),
        "keywords": msgpack_to_state_dict(x.keywords),
    },
    lambda x, sd, mismatch='error': jax.tree_util.Partial(
        x.func,
        *msgpack_from_state_dict(x.args, sd["args"], mismatch=mismatch),
        **msgpack_from_state_dict(x.keywords, sd["keywords"], mismatch=mismatch)
    )
)


# On-the-wire / disk serialization format

# We encode state-dicts via msgpack, using its custom type extension.
# https://github.com/msgpack/msgpack/blob/master/spec.md
#
# - ndarrays and DeviceArrays are serialized to nested msgpack-encoded string
#   of (shape-tuple, dtype-name (e.g. 'float32'), row-major array-bytes).
#   Note: only simple ndarray types are supported, no objects or fields.
#
# - native complex scalars are converted to nested msgpack-encoded tuples
#   (real, imag).


@set_module_as('braintools.file')
def _ndarray_to_bytes(arr) -> bytes:
    """Save ndarray to simple msgpack encoding."""
    if isinstance(arr, jax.Array):
        arr = np.array(arr)
    if arr.dtype.hasobject or arr.dtype.isalignedstruct:
        raise ValueError('Object and structured dtypes not supported '
                         'for serialization of ndarrays.')
    tpl = (arr.shape, arr.dtype.name, arr.tobytes('C'))
    return msgpack.packb(tpl, use_bin_type=True)


@set_module_as('braintools.file')
def _dtype_from_name(name: str):
    """Handle JAX bfloat16 dtype correctly."""
    if name == b'bfloat16':
        return jax.numpy.bfloat16
    else:
        return np.dtype(name)


@set_module_as('braintools.file')
def _ndarray_from_bytes(data: bytes) -> np.ndarray:
    """Load ndarray from simple msgpack encoding."""
    shape, dtype_name, buffer = msgpack.unpackb(data, raw=True)
    return np.frombuffer(buffer,
                         dtype=_dtype_from_name(dtype_name),
                         count=-1,
                         offset=0).reshape(shape, order='C')


class _MsgpackExtType(enum.IntEnum):
    __module__ = 'braintools.file'
    """Messagepack custom type ids."""
    ndarray = 1
    native_complex = 2
    npscalar = 3


@set_module_as('braintools.file')
def _msgpack_ext_pack(x):
    """Messagepack encoders for custom types."""
    # TODO: Array here only work when they are fully addressable.
    #       If they are not fully addressable, use the GDA path for checkpointing.
    if isinstance(x, (np.ndarray, jax.Array)):
        return msgpack.ExtType(_MsgpackExtType.ndarray, _ndarray_to_bytes(x))
    if issubclass(type(x), np.generic):
        # pack scalar as ndarray
        return msgpack.ExtType(
            _MsgpackExtType.npscalar,
            _ndarray_to_bytes(np.asarray(x))
        )
    elif isinstance(x, complex):
        return msgpack.ExtType(
            _MsgpackExtType.native_complex,
            msgpack.packb((x.real, x.imag))
        )
    return x


@set_module_as('braintools.file')
def _msgpack_ext_unpack(code, data):
    """Messagepack decoders for custom types."""
    if code == _MsgpackExtType.ndarray:
        return _ndarray_from_bytes(data)
    elif code == _MsgpackExtType.native_complex:
        complex_tuple = msgpack.unpackb(data)
        return complex(complex_tuple[0], complex_tuple[1])
    elif code == _MsgpackExtType.npscalar:
        ar = _ndarray_from_bytes(data)
        return ar[()]  # unpack ndarray to scalar
    return msgpack.ExtType(code, data)


@set_module_as('braintools.file')
def _np_convert_in_place(d):
    """Convert any jax devicearray leaves to numpy arrays in place."""
    if isinstance(d, dict):
        for k, v in d.items():
            if isinstance(v, jax.Array):
                d[k] = np.array(v)
            elif isinstance(v, dict):
                _np_convert_in_place(v)
    elif isinstance(d, jax.Array):
        return np.array(d)
    return d


_tuple_to_dict = lambda tpl: {str(x): y for x, y in enumerate(tpl)}
_dict_to_tuple = lambda dct: tuple(dct[str(i)] for i in range(len(dct)))


@set_module_as('braintools.file')
def _chunk(arr) -> Dict[str, Any]:
    """Convert array to a canonical dictionary of chunked arrays."""
    chunksize = max(1, int(MAX_CHUNK_SIZE / arr.dtype.itemsize))
    data = {'__msgpack_chunked_array__': True,
            'shape': _tuple_to_dict(arr.shape)}
    flatarr = arr.reshape(-1)
    chunks = [flatarr[i:i + chunksize] for i in range(0, flatarr.size, chunksize)]
    data['chunks'] = _tuple_to_dict(chunks)
    return data


@set_module_as('braintools.file')
def _unchunk(data: Dict[str, Any]):
    """Convert canonical dictionary of chunked arrays back into array."""
    assert '__msgpack_chunked_array__' in data
    shape = _dict_to_tuple(data['shape'])
    flatarr = np.concatenate(_dict_to_tuple(data['chunks']))
    return flatarr.reshape(shape)


@set_module_as('braintools.file')
def _chunk_array_leaves_in_place(d):
    """Convert oversized array leaves to safe chunked form in place."""
    if isinstance(d, dict):
        for k, v in d.items():
            if isinstance(v, np.ndarray):
                if v.size * v.dtype.itemsize > MAX_CHUNK_SIZE:
                    d[k] = _chunk(v)
            elif isinstance(v, dict):
                _chunk_array_leaves_in_place(v)
    elif isinstance(d, np.ndarray):
        if d.size * d.dtype.itemsize > MAX_CHUNK_SIZE:
            return _chunk(d)
    return d


@set_module_as('braintools.file')
def _unchunk_array_leaves_in_place(d):
    """Convert chunked array leaves back into array leaves, in place."""
    if isinstance(d, dict):
        if '__msgpack_chunked_array__' in d:
            return _unchunk(d)
        else:
            for k, v in d.items():
                if isinstance(v, dict) and '__msgpack_chunked_array__' in v:
                    d[k] = _unchunk(v)
                elif isinstance(v, dict):
                    _unchunk_array_leaves_in_place(v)
    return d


@set_module_as('braintools.file')
def _msgpack_serialize(pytree, in_place: bool = False) -> bytes:
    """Save data structure to bytes in msgpack format.

    Low-level function that only supports python trees with array leaves,
    for custom objects use `to_bytes`.  It splits arrays above MAX_CHUNK_SIZE into
    multiple chunks.

    Args:
      pytree: python tree of dict, list, tuple with python primitives
        and array leaves.
      in_place: boolean specifyng if pytree should be modified in place.

    Returns:
      msgpack-encoded bytes of pytree.
    """
    if not in_place:
        pytree = jax.tree_util.tree_map(lambda x: x, pytree)
    pytree = _np_convert_in_place(pytree)
    pytree = _chunk_array_leaves_in_place(pytree)
    return msgpack.packb(pytree, default=_msgpack_ext_pack, strict_types=True)


@set_module_as('braintools.file')
def _msgpack_restore(encoded_pytree: bytes):
    """Restore data structure from bytes in msgpack format.

    Low-level function that only supports python trees with array leaves,
    for custom objects use `from_bytes`.

    Args:
      encoded_pytree: msgpack-encoded bytes of python tree.

    Returns:
      Python tree of dict, list, tuple with python primitive
      and array leaves.
    """
    state_dict = msgpack.unpackb(encoded_pytree, ext_hook=_msgpack_ext_unpack, raw=False)
    return _unchunk_array_leaves_in_place(state_dict)


@set_module_as('braintools.file')
def _from_bytes(target, encoded_bytes: bytes, mismatch: str = 'error'):
    """Restore optimizer or other object from msgpack-serialized state-dict.

    Args:
      target: template object with state-dict registrations that matches
        the structure being deserialized from `encoded_bytes`.
      encoded_bytes: msgpack serialized object structurally isomorphic to
        `target`.  Typically, a model or optimizer.
      mismatch: How to handle mismatches between target and state dict.

    Returns:
      A new object structurally isomorphic to `target` containing the updated
      leaf data from saved data.
    """
    state_dict = _msgpack_restore(encoded_bytes)
    return msgpack_from_state_dict(target, state_dict, mismatch=mismatch)


@set_module_as('braintools.file')
def _to_bytes(target) -> bytes:
    """Save optimizer or other object as msgpack-serialized state-dict.

    Args:
      target: template object with state-dict registrations to be
        serialized to msgpack format.  Typically, a model or optimizer.

    Returns:
      Bytes of msgpack-encoded state-dict of `target` object.
    """
    state_dict = msgpack_to_state_dict(target)
    return _msgpack_serialize(state_dict, in_place=True)


# the empty node is a struct.dataclass to be compatible with JAX.
class _EmptyNode:
    __module__ = 'braintools.file'
    pass


@set_module_as('braintools.file')
def _rename_fn(src, dst, overwrite=False):
    if os.path.exists(src):
        if os.path.exists(dst) and not overwrite:
            raise AlreadyExistsError(dst)
        return os.rename(src, dst)


class AsyncManager(object):
    r"""Manage asynchronous checkpoint operations for non-blocking saves.

    This class provides a simple interface for managing asynchronous checkpoint
    operations, allowing training loops to continue while checkpoints are being
    written to disk in the background. It ensures proper sequencing and prevents
    race conditions in checkpoint operations.

    The manager uses a thread pool executor to handle async operations and
    tracks the status of save operations to maintain checkpoint consistency.

    Parameters
    ----------
    max_workers : int, default=1
        Maximum number of worker threads for async operations. For most
        use cases, 1 worker is sufficient as checkpoint saves are typically
        I/O bound rather than CPU bound.

    Examples
    --------
    Basic async checkpoint management:

    .. code-block:: python

        >>> import braintools
        >>> import jax.numpy as jnp
        >>> # Create async manager
        >>> async_mgr = braintools.file.AsyncManager(max_workers=1)
        >>> # Model state to save
        >>> model_state = {'weights': jnp.ones((100, 100)), 'step': 1000}
        >>> # Start async save
        >>> braintools.file.msgpack_save(
        ...     'checkpoint_1000.msgpack', model_state,
        ...     async_manager=async_mgr, verbose=False
        ... )
        >>> # Training continues immediately
        >>> print("Training continues while checkpoint saves...")
        >>> # Wait for completion before next checkpoint
        >>> async_mgr.wait_previous_save()
        >>> print("Checkpoint complete!")

    Training loop with async checkpointing:

    .. code-block:: python

        >>> async_mgr = braintools.file.AsyncManager()
        >>> for step in range(1000, 5000, 1000):
        ...     # Update model state
        ...     model_state['step'] = step
        ...     # Async save (non-blocking)
        ...     braintools.file.msgpack_save(
        ...         f'checkpoint_{step}.msgpack', model_state,
        ...         async_manager=async_mgr, verbose=False
        ...     )
        ...     # Continue training immediately
        ...     print(f"Step {step}: training continues...")
        >>> # Wait for final save to complete
        >>> async_mgr.wait_previous_save()

    See Also
    --------
    msgpack_save : Save checkpoints with optional async support
    msgpack_load : Load checkpoints

    Notes
    -----
    **Thread safety**: The manager ensures that only one save operation
    runs at a time, preventing race conditions and maintaining checkpoint
    consistency.

    **Memory considerations**: While saves run asynchronously, the data
    to be saved is captured at the time `msgpack_save` is called, so
    memory usage may be higher during async operations.

    **Error handling**: If an async save fails, the error will be raised
    when `wait_previous_save()` is called or when the next save is attempted.

    **Resource cleanup**: The thread pool executor is automatically managed
    and cleaned up when the AsyncManager instance is garbage collected.
    """
    __module__ = 'braintools.file'

    def __init__(self, max_workers: int = 1):
        self.executor = thread.ThreadPoolExecutor(max_workers=max_workers)
        self.save_future = None

    def wait_previous_save(self):
        """Block until the previous save finishes, to keep files' consistency."""
        if self.save_future and not self.save_future.done():
            warnings.warn(
                'The previous async brainpy.checkpoints.save has not finished yet. Waiting '
                'for it to complete before the next save.',
                UserWarning
            )
            self.save_future.result()

    def save_async(self, task: Callable[[], Any]):
        """Run a task async. The future will be tracked as self.save_future.

        Args:
          task: The callable to be executed asynchrously.
        """
        self.wait_previous_save()
        self.save_future = self.executor.submit(task)  # type: ignore


@set_module_as('braintools.file')
def _save_commit(
    filename: str,
    overwrite: bool,
) -> None:
    """Commit changes after saving checkpoints to disk.

    This function does the following, sequentially:
      1. Make sure all ckpt writing finishes, and rename them from temp path to
      the final path.
      2. Remove newer checkpoints (files that ordered larger than this save) if
      `overwrite=True`.
      3. Remove old checkpoint files based on `keep` and `keep_every_n_steps`.
      4. Record program duration saved by this checkpoint.
    """
    ckpt_path = os.path.dirname(filename)
    ckpt_tmp_path = os.path.join(ckpt_path, 'tmp')
    _rename_fn(ckpt_tmp_path, ckpt_path, overwrite=overwrite)


@set_module_as('braintools.file')
def _save_main_ckpt_file(
    target: bytes,
    filename: str,
    overwrite: bool,
):
    """Save the main checkpoint file via file system."""
    with open(filename, 'wb') as fp:
        fp.write(target)
    # Postpone the commitment of checkpoint to after MPA writes are done.
    _save_commit(filename, overwrite)


@set_module_as('braintools.file')
def msgpack_save(
    filename: str,
    target: brainstate.typing.PyTree,
    overwrite: bool = True,
    async_manager: Optional[AsyncManager] = None,
    verbose: bool = True,
) -> None:
    r"""Save a checkpoint using the msgpack serialization format.

    This function provides efficient, pre-emption safe checkpointing suitable
    for single-host scenarios. It serializes Python objects to msgpack format,
    which offers fast serialization and deserialization with excellent
    compression for numerical data.

    The save operation is atomic - it writes to a temporary location first,
    then renames to the final location to prevent corruption from interrupted
    saves. Optional async support allows non-blocking saves.

    Parameters
    ----------
    filename : str
        Path where the checkpoint file will be saved. Parent directories
        will be created automatically if they don't exist.
    target : brainstate.typing.PyTree
        The object to be serialized and saved. Can be any Python tree
        structure containing arrays, dictionaries, lists, tuples, and
        custom registered types.
    overwrite : bool, default=True
        Whether to overwrite existing checkpoint files. If False and
        the target file exists, raises `InvalidCheckpointPath`.
    async_manager : AsyncManager, optional
        If provided, the save operation runs asynchronously without
        blocking the main thread. Only works for single-host scenarios.
        Ongoing saves will block subsequent saves to ensure consistency.
    verbose : bool, default=True
        Whether to print progress information during the save operation.

    Raises
    ------
    ModuleNotFoundError
        If the msgpack library is not installed.
    InvalidCheckpointPath
        If `overwrite` is False and the target file already exists.

    Examples
    --------
    Basic checkpoint saving:

    .. code-block:: python

        >>> import braintools
        >>> import jax.numpy as jnp
        >>> # Create model state
        >>> model_state = {
        ...     'weights': jnp.array([[1.0, 2.0], [3.0, 4.0]]),
        ...     'bias': jnp.array([0.1, 0.2]),
        ...     'step': 1000
        ... }
        >>> # Save checkpoint
        >>> braintools.file.msgpack_save('checkpoint.msgpack', model_state)
        Saving checkpoint into checkpoint.msgpack

    Async saving for non-blocking operation:

    .. code-block:: python

        >>> # Setup async manager
        >>> async_mgr = braintools.file.AsyncManager(max_workers=2)
        >>> # Save asynchronously
        >>> braintools.file.msgpack_save(
        ...     'checkpoint_async.msgpack', model_state,
        ...     async_manager=async_mgr, verbose=False
        ... )
        >>> # Continue training while save happens in background
        >>> # ... training code ...
        >>> # Wait for save to complete before next checkpoint
        >>> async_mgr.wait_previous_save()

    Save with custom file path:

    .. code-block:: python

        >>> import os
        >>> checkpoint_dir = 'checkpoints/experiment_1'
        >>> step = 5000
        >>> filename = os.path.join(checkpoint_dir, f'step_{step}.msgpack')
        >>> braintools.file.msgpack_save(filename, model_state, overwrite=True)

    See Also
    --------
    msgpack_load : Load checkpoint from msgpack file
    AsyncManager : Manage asynchronous checkpoint operations
    msgpack_register_serialization : Register custom types for serialization

    Notes
    -----
    **Multi-process considerations**: This function is designed for single-host
    scenarios. For multi-process checkpointing to shared storage (e.g., cloud
    buckets) or saving `GlobalDeviceArray`s, use JAX's native checkpointing
    utilities instead.

    **Pre-emption safety**: The function writes to a temporary file first,
    then atomically renames it to the final location, ensuring that partial
    writes don't corrupt existing checkpoints.

    **Memory efficiency**: Large arrays are automatically chunked to work
    around msgpack's size limitations, enabling serialization of very large
    models.

    **Async behavior**: When using `async_manager`, the final file commitment
    happens in an async callback. Use `async_manager.wait_previous_save()`
    to ensure completion before critical operations.
    """
    check_msgpack()
    if verbose:
        print(f'Saving checkpoint into {filename}')

    # Make sure all saves are finished before the logic
    # of checking and removing outdated checkpoints happens.
    if async_manager:
        async_manager.wait_previous_save()

    if os.path.dirname(filename):
        os.makedirs(os.path.dirname(filename), exist_ok=True)
    if not overwrite and os.path.exists(filename):
        raise InvalidCheckpointPath(filename)

    if isinstance(target, brainstate.util.FlattedDict):
        target = target.to_nest()
    target = _to_bytes(target)

    # Save the files via I/O sync or async.
    def save_main_ckpt_task():
        return _save_main_ckpt_file(target, filename, overwrite)

    if async_manager:
        async_manager.save_async(save_main_ckpt_task)
    else:
        save_main_ckpt_task()


@set_module_as('braintools.file')
def msgpack_load(
    filename: str,
    target: Optional[Any] = None,
    parallel: bool = True,
    mismatch: str = 'error',
    verbose: bool = True,
) -> brainstate.typing.PyTree:
    r"""Load a checkpoint from msgpack format.

    This function efficiently loads and deserializes checkpoints saved with
    `msgpack_save`. It supports both standalone deserialization (returning
    raw data) and template-based restoration (updating an existing object
    structure with saved state).

    The loading process includes automatic array unchunking, parallel I/O
    for large files, and flexible mismatch handling for robust checkpoint
    compatibility.

    Parameters
    ----------
    filename : str
        Path to the checkpoint file to load. Must be a valid msgpack file
        created by `msgpack_save`.
    target : Any, optional
        Template object to restore state into. If provided, the function
        returns a copy of this object with state updated from the checkpoint.
        If None, returns the raw deserialized data as a dictionary.
    parallel : bool, default=True
        Whether to use parallel I/O for loading large files. Improves
        performance for seekable files by reading chunks concurrently.
        Set to False for non-seekable files or debugging.
    mismatch : {'error', 'warn', 'ignore'}, default='error'
        How to handle structural mismatches between target and checkpoint:

        - ``'error'``: Raise ValueError on any mismatch
        - ``'warn'``: Issue warning and skip mismatched keys
        - ``'ignore'``: Silently skip mismatched keys
    verbose : bool, default=True
        Whether to print loading progress information.

    Returns
    -------
    brainstate.typing.PyTree
        If `target` is provided: A copy of the target object with state
        restored from the checkpoint.
        If `target` is None: The raw deserialized data structure.

    Raises
    ------
    ModuleNotFoundError
        If the msgpack library is not installed.
    ValueError
        If the checkpoint file is not found.
    ValueError
        If `mismatch='error'` and structural mismatches are detected.

    Examples
    --------
    Load checkpoint without target (raw data):

    .. code-block:: python

        >>> import braintools
        >>> # Load raw checkpoint data
        >>> raw_data = braintools.file.msgpack_load('checkpoint.msgpack')
        >>> print(type(raw_data))
        <class 'dict'>
        >>> print(raw_data.keys())
        dict_keys(['weights', 'bias', 'step'])

    Load checkpoint into existing object:

    .. code-block:: python

        >>> import jax.numpy as jnp
        >>> # Create target template
        >>> template = {
        ...     'weights': jnp.zeros((2, 2)),
        ...     'bias': jnp.zeros(2),
        ...     'step': 0
        ... }
        >>> # Load and restore state
        >>> restored = braintools.file.msgpack_load(
        ...     'checkpoint.msgpack', target=template
        ... )
        Loading checkpoint from checkpoint.msgpack
        >>> print(restored['step'])
        1000

    Handle mismatched structures gracefully:

    .. code-block:: python

        >>> # Template missing some keys
        >>> partial_template = {'weights': jnp.zeros((2, 2))}
        >>> # Load with warning on mismatch
        >>> restored = braintools.file.msgpack_load(
        ...     'checkpoint.msgpack', target=partial_template,
        ...     mismatch='warn', verbose=False
        ... )

    Load large checkpoint with optimized I/O:

    .. code-block:: python

        >>> # For very large files, parallel loading improves speed
        >>> large_model = braintools.file.msgpack_load(
        ...     'large_checkpoint.msgpack', parallel=True
        ... )

    See Also
    --------
    msgpack_save : Save checkpoint to msgpack file
    msgpack_from_state_dict : Low-level state restoration
    msgpack_to_state_dict : Low-level state extraction

    Notes
    -----
    **Performance**: The function uses optimized parallel I/O for files larger
    than 128MB when `parallel=True`. For smaller files or non-seekable streams,
    standard sequential reading is used.

    **Memory efficiency**: Large arrays are automatically unchunked during
    loading, allowing seamless restoration of very large models that were
    chunked during saving.

    **Compatibility**: The mismatch handling options provide flexibility when
    loading checkpoints into slightly different model architectures, useful
    for transfer learning or model evolution scenarios.

    **Thread safety**: Parallel loading uses a thread pool with up to 32
    workers for optimal I/O performance on modern storage systems.
    """
    check_msgpack()

    if not os.path.exists(filename):
        raise ValueError(f'Checkpoint not found: {filename}')
    if verbose:
        sys.stdout.write(f'Loading checkpoint from {filename}\n')
        sys.stdout.flush()
    file_size = os.path.getsize(filename)

    with open(filename, 'rb') as fp:
        if parallel and fp.seekable():
            buf_size = 128 << 20  # 128M buffer.
            num_bufs = file_size / buf_size
            checkpoint_contents = bytearray(file_size)

            def read_chunk(i):
                # NOTE: We have to re-open the file to read each chunk, otherwise the
                # parallelism has no effect. But we could reuse the file pointers
                # within each thread.
                with open(filename, 'rb') as f:
                    f.seek(i * buf_size)
                    buf = f.read(buf_size)
                    if buf:
                        checkpoint_contents[i * buf_size:i * buf_size + len(buf)] = buf
                    return len(buf) / buf_size

            pool_size = 32
            pool = thread.ThreadPoolExecutor(pool_size)
            results = pool.map(read_chunk, range(int(num_bufs) + 1))
            pool.shutdown(wait=False)
            wait = list(results)
        else:
            checkpoint_contents = fp.read()

    state_dict = _msgpack_restore(checkpoint_contents)
    if target is not None:
        state_dict = msgpack_from_state_dict(target, state_dict, mismatch=mismatch)

    return state_dict
